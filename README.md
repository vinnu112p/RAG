<div align="center">
  <h1>ğŸ“š RAG Pipeline: Complete Guide from Ingestion to Retrieval</h1>
  <p>
    <b>A comprehensive Retrieval-Augmented Generation (RAG) system built with Python, LangChain, and ChromaDB</b>
  </p>
  <p>
    <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python Version">
    <img src="https://img.shields.io/badge/LangChain-Framework-green.svg" alt="LangChain">
    <img src="https://img.shields.io/badge/ChromaDB-Vector%20Store-orange.svg" alt="ChromaDB">
    <img src="https://img.shields.io/badge/SentenceTransformers-Embeddings-yellow.svg" alt="Embeddings">
  </p>
</div>

<hr>

## ğŸ“– Table of Contents
- [What is RAG?](#-what-is-rag)
- [Project Architecture](#ï¸-project-architecture)
- [Key Modules & Why We Use Them](#ï¸-key-modules--why-we-use-them)
- [Detailed Code Walkthrough](#-detailed-code-walkthrough)
- [Folder Structure](#-folder-structure)
- [Tech Stack](#-tech-stack)
- [Setup & Installation](#-setup--installation)
- [How to Use](#-how-to-use)

<hr>

## ğŸ¤– What is RAG?

**RAG (Retrieval-Augmented Generation)** is an AI technique that combines two powerful capabilities:
1. **Retrieval**: Finding relevant information from a knowledge base
2. **Generation**: Using that information to generate accurate responses

### Why RAG?
Traditional Large Language Models (LLMs) have two major limitations:
- âŒ **Knowledge Cutoff**: They only know information up to their training date
- âŒ **Hallucinations**: They sometimes generate incorrect information confidently

RAG solves these problems by:
- âœ… Fetching **real-time, domain-specific data** from your own documents
- âœ… Grounding responses in **actual source material**, reducing hallucinations
- âœ… Making AI systems **verifiable** - you can trace answers back to source documents

### How Does RAG Work?

Think of RAG like a smart research assistant:
1. **You have a library** (your documents/PDFs)
2. **You ask a question** ("What are the benefits of exception handling?")
3. **The assistant searches** the library for relevant sections
4. **The assistant reads** those sections and answers your question based on what it found

<div align="center">
  <table>
    <tr>
      <td align="center"><b>ğŸ“¥ Data Ingestion</b></td>
      <td align="center">â†’</td>
      <td align="center"><b>âœ‚ï¸ Chunking</b></td>
      <td align="center">â†’</td>
      <td align="center"><b>ğŸ§® Embedding</b></td>
      <td align="center">â†’</td>
      <td align="center"><b>ğŸ’¾ Vector Store</b></td>
      <td align="center">â†’</td>
      <td align="center"><b>ğŸ” Retrieval</b></td>
    </tr>
  </table>
</div>

<hr>

## ğŸ—ï¸ Project Architecture

This project implements the complete RAG pipeline step-by-step:

### 1. **Data Ingestion** ğŸ“¥
- **What**: Loading raw documents (PDFs, text files) into Python
- **Why**: We need to bring external knowledge into our system
- **How**: Using LangChain's document loaders (`TextLoader`, `PyMuPDFLoader`)

### 2. **Text Chunking** âœ‚ï¸
- **What**: Splitting large documents into smaller pieces (chunks)
- **Why**: 
  - LLMs have input size limits
  - Smaller chunks make retrieval more precise
  - We want to find the *specific paragraph* that answers the question, not the entire 100-page document
- **How**: Using `RecursiveCharacterTextSplitter` with:
  - **Chunk Size**: 1000 characters per chunk
  - **Chunk Overlap**: 200 characters overlap between chunks (preserves context across boundaries)

### 3. **Embedding Generation** ğŸ§®
- **What**: Converting text into numerical vectors (arrays of numbers)
- **Why**: Computers can't directly compare "meaning" of text, but they can compare vectors using math
- **Example**: 
  - "Python programming" â†’ `[0.23, -0.45, 0.67, ...]` (384 numbers)
  - "Coding in Python" â†’ `[0.25, -0.43, 0.69, ...]` (very similar numbers!)
- **How**: Using `SentenceTransformer` model (`all-MiniLM-L6-v2`)

### 4. **Vector Storage** ğŸ’¾
- **What**: Saving embeddings in a specialized database
- **Why**: We need fast similarity search across thousands/millions of vectors
- **How**: Using ChromaDB - a persistent vector database

### 5. **Retrieval** ğŸ”
- **What**: Finding the most relevant chunks for a user's query
- **Why**: We only want to show the LLM the top 5 most relevant passages, not all 10,000 chunks
- **How**: Convert query to embedding â†’ Find nearest neighbors using cosine similarity

<hr>

## ğŸ› ï¸ Key Modules & Why We Use Them

Here's a detailed explanation of every library and module used in this project:

<table>
  <thead>
    <tr>
      <th>Module / Library</th>
      <th>Purpose</th>
      <th>Detailed Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b><code>langchain_core.documents</code></b></td>
      <td>Document Structure</td>
      <td>Provides the <code>Document</code> class which is the foundation for storing text data along with metadata (source file, page number, author, etc.). Every piece of text in our system is wrapped in this format.</td>
    </tr>
    <tr>
      <td><b><code>langchain_community.document_loaders</code></b></td>
      <td>File Loading</td>
      <td>
        Contains specialized loaders for different file types:<br>
        â€¢ <code>TextLoader</code>: Reads .txt files with proper encoding<br>
        â€¢ <code>DirectoryLoader</code>: Batch loads all files matching a pattern<br>
        â€¢ <code>PyMuPDFLoader</code>: Extracts text from PDFs while preserving page numbers and metadata
      </td>
    </tr>
    <tr>
      <td><b><code>langchain_text_splitters</code></b></td>
      <td>Text Chunking</td>
      <td>
        <code>RecursiveCharacterTextSplitter</code> intelligently splits text by:<br>
        1. First trying to split on paragraphs (<code>\n\n</code>)<br>
        2. Then sentences (<code>\n</code>)<br>
        3. Then words (<code> </code>)<br>
        4. Finally characters if needed<br>
        This preserves natural language structure better than random character splits.
      </td>
    </tr>
    <tr>
      <td><b><code>sentence_transformers</code></b></td>
      <td>Embedding Model</td>
      <td>
        Uses pre-trained neural networks to convert text into dense vectors:<br>
        â€¢ Model: <code>all-MiniLM-L6-v2</code> (384 dimensions)<br>
        â€¢ Fast and efficient (6 layers)<br>
        â€¢ Works well for semantic similarity tasks<br>
        â€¢ Pre-trained on millions of sentence pairs from the internet
      </td>
    </tr>
    <tr>
      <td><b><code>chromadb</code></b></td>
      <td>Vector Database</td>
      <td>
        A specialized database for storing and querying embeddings:<br>
        â€¢ <b>Persistent storage</b>: Saves to disk, no need to re-embed each time<br>
        â€¢ <b>Fast similarity search</b>: Uses approximate nearest neighbor algorithms<br>
        â€¢ <b>Collections</b>: Organizes embeddings into groups (like database tables)<br>
        â€¢ Automatically handles distance calculations (cosine, euclidean, etc.)
      </td>
    </tr>
    <tr>
      <td><b><code>uuid</code></b></td>
      <td>Unique Identifiers</td>
      <td>
        Generates universally unique IDs for each document chunk:<br>
        â€¢ Format: <code>doc_a3f5b89c_42</code><br>
        â€¢ Ensures no two chunks have the same ID, even across multiple runs<br>
        â€¢ Required by ChromaDB for tracking documents
      </td>
    </tr>
    <tr>
      <td><b><code>sklearn.metrics.pairwise</code></b></td>
      <td>Similarity Calculation</td>
      <td>
        <code>cosine_similarity</code> measures how similar two vectors are:<br>
        â€¢ Returns a score from -1 to 1<br>
        â€¢ 1 = identical meaning<br>
        â€¢ 0 = unrelated<br>
        â€¢ -1 = opposite meaning
      </td>
    </tr>
  </tbody>
</table>

<hr>

## ğŸ’» Detailed Code Walkthrough

Let's break down each major component of the code in `notebook/document.ipynb`:

### ğŸ“Œ Part 1: Data Ingestion

#### Creating Sample Documents
```python
doc = Document(
    page_content="this is the main text content",
    metadata={"source": "example.txt", "author": "predator"}
)
```
**What's happening?**
- We create a `Document` object (LangChain's standard format)
- `page_content`: The actual text we want to store
- `metadata`: Extra information (where it came from, who wrote it, etc.)

#### Loading Text Files
```python
loader = TextLoader("../data/txt_files/python_intro.txt", encoding="utf-8")
documents = loader.load()
```
**What's happening?**
- `TextLoader` reads the file
- `encoding="utf-8"` ensures special characters (emojis, non-English text) are handled correctly
- Returns a list of `Document` objects (one per file)

#### Loading PDFs
```python
for path in glob.glob("../data/pdfs/*.pdf"):
    loader = PyMuPDFLoader(path)
    docs = loader.load()
    for d in docs:
        d.metadata["source"] = path  # ğŸ”‘ CRITICAL!
    pdf_documents.extend(docs)
```
**What's happening?**
- `glob.glob` finds all PDF files in the folder
- `PyMuPDFLoader` extracts text from each page
- **Important**: We manually set `metadata["source"]` so we can trace answers back to the original PDF
- Result: 187 documents (one per page across all PDFs)

---

### ğŸ“Œ Part 2: Text Splitting

```python
def split_documents(documents, chunk_size=1000, chunk_overlap=200):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    split_docs = text_splitter.split_documents(documents)
    return split_docs
```

**What's happening?**
- **`chunk_size=1000`**: Each chunk is ~1000 characters (roughly 2-3 paragraphs)
- **`chunk_overlap=200`**: Last 200 characters of Chunk A are repeated at the start of Chunk B
  - **Why?** If a sentence is cut between chunks, it still appears complete in one chunk
- **`separators`**: Try to split at paragraph boundaries first, then newlines, then spaces
- **Result**: 187 documents â†’ 481 chunks (smaller, more precise pieces)

---

### ğŸ“Œ Part 3: Embedding Generation

#### The `EmbeddingManager` Class
```python
class EmbeddingManager:
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
    
    def generate_embeddings(self, texts):
        embeddings = self.model.encode(texts, show_progress_bar=True)
        return embeddings
```

**What's happening?**
- **`__init__`**: Downloads and loads the embedding model (first run only, then cached)
- **`generate_embeddings`**: 
  - Takes a list of text strings
  - Returns a NumPy array of shape `(num_texts, 384)`
  - Each text becomes a 384-dimensional vector

**Example**:
```python
texts = ["Python is great", "I love programming"]
embeddings = embedding_manager.generate_embeddings(texts)
# Shape: (2, 384)
# embeddings[0] = [0.12, -0.45, 0.67, ...]  â† "Python is great"
# embeddings[1] = [0.15, -0.42, 0.71, ...]  â† "I love programming"
```

---

### ğŸ“Œ Part 4: Vector Store

#### The `VectorStore` Class
```python
class VectorStore:
    def __init__(self, collection_name="pdf_documents", 
                 persist_directory="../data/vector_store"):
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name=collection_name
        )
    
    def add_documents(self, documents, embeddings):
        ids = [f"doc_{uuid.uuid4().hex[:8]}_{i}" for i in range(len(documents))]
        metadatas = [dict(doc.metadata) for doc in documents]
        documents_text = [doc.page_content for doc in documents]
        embeddings_list = [emb.tolist() for emb in embeddings]
        
        self.collection.add(
            ids=ids,
            embeddings=embeddings_list,
            metadatas=metadatas,
            documents=documents_text
        )
```

**What's happening?**
- **`PersistentClient`**: Creates/opens a database at `../data/vector_store`
  - Data persists between runs (no need to re-embed every time!)
- **`get_or_create_collection`**: Like creating a table in SQL
- **`add_documents`**:
  - Generates unique IDs for each chunk
  - Stores 4 things: ID, embedding vector, metadata, original text
  - ChromaDB automatically indexes the embeddings for fast search

---

### ğŸ“Œ Part 5: Retrieval

#### The `RAGRetriever` Class
```python
class RAGRetriever:
    def retrieve(self, query, top_k=5, score_threshold=0.0):
        # Step 1: Convert query to embedding
        query_embedding = self.embedding_manager.generate_embeddings([query])[0]
        
        # Step 2: Search vector store
        results = self.vector_store.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )
        
        # Step 3: Process results
        retrieved_docs = []
        for i, (doc_id, document, metadata, distance) in enumerate(...):
            similarity_score = 1 - distance  # Convert distance to similarity
            if similarity_score >= score_threshold:
                retrieved_docs.append({
                    'content': document,
                    'metadata': metadata,
                    'similarity_score': similarity_score
                })
        
        return retrieved_docs
```

**What's happening?**
1. **Query â†’ Embedding**: Convert your question to a vector (same 384 dimensions)
2. **Vector Search**: ChromaDB finds the 5 closest vectors using cosine distance
3. **Score Conversion**: Distance is converted to similarity (higher = better match)
4. **Filtering**: Only return results above the threshold

**Example Query Flow**:
```
User Query: "What are the benefits of exception handling?"
           â†“
Embedding:  [0.34, -0.12, 0.89, ...] (384 numbers)
           â†“
ChromaDB Search: Find nearest 5 vectors
           â†“
Results:   
  1. "Exception handling provides..." (similarity: 0.92)
  2. "Benefits of try-catch blocks..." (similarity: 0.87)
  3. ...
```

---

## ğŸ“‚ Folder Structure

```
RAG/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“ pdfs/                    # Your source PDF documents
â”‚   â”‚   â”œâ”€â”€ JavaInterviewQuestions.pdf
â”‚   â”‚   â”œâ”€â”€ EngineeringPhysics.pdf
â”‚   â”‚   â””â”€â”€ ExceptionHandling.pdf
â”‚   â”œâ”€â”€ ğŸ“ txt_files/               # Your source text files
â”‚   â”‚   â”œâ”€â”€ python_intro.txt
â”‚   â”‚   â””â”€â”€ ml_intro.txt
â”‚   â””â”€â”€ ğŸ“ vector_store/            # ChromaDB persistent storage
â”‚       â”œâ”€â”€ chroma.sqlite3          # Database file
â”‚       â””â”€â”€ ...                     # Index files
â”‚
â”œâ”€â”€ ğŸ“ notebook/
â”‚   â””â”€â”€ document.ipynb              # Main RAG pipeline (Jupyter Notebook)
â”‚
â”œâ”€â”€ ğŸ“ .venv/                       # Virtual environment (isolated Python packages)
â”‚
â”œâ”€â”€ .gitignore                      # Git ignore file
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ pyproject.toml                  # Project configuration
â””â”€â”€ README.md                       # This file!
```

---

## ğŸ”§ Tech Stack

| Technology | Purpose |
|:-----------|:--------|
| **Python 3.10+** | Programming language |
| **LangChain** | RAG framework & document processing |
| **ChromaDB** | Vector database for embeddings |
| **SentenceTransformers** | Neural embedding model |
| **PyMuPDF** | PDF text extraction |
| **Jupyter Notebook** | Interactive development environment |

---

## ğŸš€ Setup & Installation

### Prerequisites
- Python 3.10 or higher installed on your system
- Git (for cloning the repository)

### Step 1: Clone the Repository
```bash
git clone <your-repository-url>
cd RAG
```

### Step 2: Create a Virtual Environment

A **virtual environment** is an isolated Python environment for your project. It prevents dependency conflicts between different projects.

#### On Windows:
```bash
# Create virtual environment
python -m venv .venv

# Activate it
.venv\Scripts\activate

# You'll see (.venv) appear in your terminal prompt
```

#### On Mac/Linux:
```bash
# Create virtual environment
python3 -m venv .venv

# Activate it
source .venv/bin/activate

# You'll see (.venv) appear in your terminal prompt
```

**What just happened?**
- `python -m venv .venv`: Creates a new folder `.venv` containing a copy of Python and pip
- `activate`: Switches your terminal to use THIS copy of Python
- Now when you run `pip install`, packages go into `.venv`, not your system Python

**To deactivate later:**
```bash
deactivate
```

### Step 3: Install Dependencies
```bash
# Make sure your virtual environment is activated!
pip install -r requirements.txt
```

This installs:
- `langchain`
- `langchain-community`
- `chromadb`
- `sentence-transformers`
- `pymupdf`
- And all their dependencies

### Step 4: Launch Jupyter Notebook
```bash
jupyter notebook
```
- This opens a web browser
- Navigate to `notebook/document.ipynb`
- Run cells sequentially (Shift+Enter)

---

## ğŸ¯ How to Use

### Running the Complete Pipeline

1. **Activate your virtual environment**:
   ```bash
   .venv\Scripts\activate  # Windows
   source .venv/bin/activate  # Mac/Linux
   ```

2. **Open the notebook**:
   ```bash
   jupyter notebook notebook/document.ipynb
   ```

3. **Run all cells in order** (Cell â†’ Run All)

4. **Query your documents**:
   ```python
   # At the end of the notebook
   results = rag_retriever.retrieve("Your question here")
   
   # Print clean results
   for doc in results:
       print(f"Content: {doc['content']}")
       print(f"Source: {doc['metadata']['source']}")
       print(f"Similarity: {doc['similarity_score']:.2f}")
       print("-" * 50)
   ```

### Adding Your Own Documents

1. **For PDFs**: Drop them in `data/pdfs/`
2. **For text files**: Drop them in `data/txt_files/`
3. **Re-run the notebook** to ingest and embed the new documents

### Understanding Chunk Overlap

If you notice your retrieval results include text from "before" your topic:
- This is **by design** due to `chunk_overlap=200`
- The overlap preserves context across chunk boundaries
- You can reduce it by changing `chunk_overlap` in the `split_documents` function
- Trade-off: Less overlap = faster, but might lose context

---

<div align="center">
  <h3>ğŸ‰ You now have a fully functional RAG system!</h3>
  <p><i>Built with â¤ï¸ for learning and exploration</i></p>
</div>

<hr>

## ğŸ“ License
This project is open source and available for educational purposes.

## ğŸ¤ Contributing
Feel free to fork, modify, and submit pull requests!

## ğŸ“§ Contact
Questions? Feedback? Open an issue on GitHub!
