{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf3e07b",
   "metadata": {},
   "source": [
    "## Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2d1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb115363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'predator', 'date_created': '2025-12-24'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata = {\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\":\"predator\",\n",
    "        \"date_created\":\"2025-12-24\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6d3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a simple directory\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/txt_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9728e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text file created\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/txt_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "    \n",
    "Python is a high-level, interpreted, general-purpose programming language designed to be simple, readable, and powerful.\n",
    "\n",
    "Key features :\n",
    "\n",
    "üß† Easy to learn & read ‚Äì clear, English-like syntax\n",
    "\n",
    "‚ö° Interpreted ‚Äì runs code line by line, no compilation\n",
    "\n",
    "üåç Cross-platform ‚Äì works on Windows, macOS, Linux\n",
    "\n",
    "üìö Large standard library ‚Äì built-in tools for many tasks\n",
    "\n",
    "üîå Rich ecosystem ‚Äì thousands of third-party libraries\n",
    "\n",
    "üß© Object-oriented & functional ‚Äì supports multiple paradigms\n",
    "\n",
    "üöÄ Versatile ‚Äì used in web, data science, AI, automation, and more\n",
    "\n",
    "    \"\"\",\n",
    "\n",
    "     \"../data/txt_files/ml_intro.txt\":\"\"\" Machine Learning Introduction\n",
    "\n",
    "    Machine Learning (ML) is a branch of artificial intelligence that enables computers to learn from data and improve performance without being explicitly programmed.\n",
    "\n",
    "Key features\n",
    "\n",
    "üìä Data-driven ‚Äì learns patterns from data\n",
    "\n",
    "ü§ñ Self-improving ‚Äì performance improves with more data\n",
    "\n",
    "üß† Predictive ‚Äì makes predictions or decisions\n",
    "\n",
    "üîÅ Automated learning ‚Äì reduces manual rule-based coding\n",
    "\n",
    "üìà Scalable ‚Äì works with large and complex datasets\n",
    "\n",
    "üåê Wide applications ‚Äì used in vision, speech, recommendation, fraud detection\n",
    "\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)    \n",
    "\n",
    "print(\"Sample text file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7c832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/txt_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted, general-purpose programming language designed to be simple, readable, and powerful.\\n\\nKey features :\\n\\nüß† Easy to learn & read ‚Äì clear, English-like syntax\\n\\n‚ö° Interpreted ‚Äì runs code line by line, no compilation\\n\\nüåç Cross-platform ‚Äì works on Windows, macOS, Linux\\n\\nüìö Large standard library ‚Äì built-in tools for many tasks\\n\\nüîå Rich ecosystem ‚Äì thousands of third-party libraries\\n\\nüß© Object-oriented & functional ‚Äì supports multiple paradigms\\n\\nüöÄ Versatile ‚Äì used in web, data science, AI, automation, and more\\n\\n    ')]\n"
     ]
    }
   ],
   "source": [
    "###TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/txt_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "print(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc1b435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\txt_files\\\\ml_intro.txt'}, page_content=' Machine Learning Introduction\\n\\n    Machine Learning (ML) is a branch of artificial intelligence that enables computers to learn from data and improve performance without being explicitly programmed.\\n\\nKey features\\n\\nüìä Data-driven ‚Äì learns patterns from data\\n\\nü§ñ Self-improving ‚Äì performance improves with more data\\n\\nüß† Predictive ‚Äì makes predictions or decisions\\n\\nüîÅ Automated learning ‚Äì reduces manual rule-based coding\\n\\nüìà Scalable ‚Äì works with large and complex datasets\\n\\nüåê Wide applications ‚Äì used in vision, speech, recommendation, fraud detection\\n\\n'),\n",
       " Document(metadata={'source': '..\\\\data\\\\txt_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted, general-purpose programming language designed to be simple, readable, and powerful.\\n\\nKey features :\\n\\nüß† Easy to learn & read ‚Äì clear, English-like syntax\\n\\n‚ö° Interpreted ‚Äì runs code line by line, no compilation\\n\\nüåç Cross-platform ‚Äì works on Windows, macOS, Linux\\n\\nüìö Large standard library ‚Äì built-in tools for many tasks\\n\\nüîå Rich ecosystem ‚Äì thousands of third-party libraries\\n\\nüß© Object-oriented & functional ‚Äì supports multiple paradigms\\n\\nüöÄ Versatile ‚Äì used in web, data science, AI, automation, and more\\n\\n    ')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Directory Loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## Load all text files from the directory\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/txt_files\",\n",
    "     glob=\"**/*.txt\", ## pattern to match file\n",
    "     loader_cls = TextLoader, ## loader class to use\n",
    "     loader_kwargs={'encoding':'utf-8'},\n",
    "     show_progress=False\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e30552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDFs: Counter({'../data/pdfs\\\\Dbms_Sql.pdf': 173, '../data/pdfs\\\\Vinnu_ka_DAA.pdf': 20, '../data/pdfs\\\\6-7Os.pdf': 3})\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import glob\n",
    "\n",
    "pdf_documents = []\n",
    "\n",
    "for path in glob.glob(\"../data/pdfs/*.pdf\"):\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = path   # üîë REQUIRED\n",
    "    pdf_documents.extend(docs)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Loaded PDFs:\", Counter(d.metadata[\"source\"] for d in pdf_documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "021a0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better RAG performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunk_size: Maximum characters per chunk (adjust based on your LLM)\n",
    "    - chunk_overlap: Characters to overlap between chunks (preserves context)\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, # Each chunk: ~1000 characters\n",
    "        chunk_overlap=chunk_overlap, # 200 chars overlap for context\n",
    "        length_function=len, # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Split hierarchy\n",
    "    )\n",
    "    # Actually split the documents\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show what a chunk looks like\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e7776",
   "metadata": {},
   "source": [
    "##Embedding and VectorStoreDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89fe24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe301d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension : 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x29d322d06e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self,model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "            Initialize the embedding manager\n",
    "\n",
    "            Args:\n",
    "                model_name : Hugging face Model name for sentence embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence Transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model =    SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension : {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self,text: List[str])->np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embedding for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts:LIst of text strings to embed\n",
    "\n",
    "        Return :\n",
    "            numpy array of embedding with shape (len(texts),embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape : {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ### Initialize the embedding manager\n",
    "\n",
    "embedding_manager= EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de0c7c",
   "metadata": {},
   "source": [
    "## Vector StoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89fc99eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Initialized. Collections: pdf_documents\n",
      "Existing documents in collections   : 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x29d3464e510>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\" manages document embeddings in a chromaDB vector store \"\"\"\n",
    "\n",
    "    def __init__(self,collection_name: str= \"pdf_documents\", persist_directory: str= \"../data/vector_store\"):\n",
    "        \"\"\"  \n",
    "        Initialize the vector store\n",
    "\n",
    "        Args:\n",
    "            collection_name: Name of chromaDB collection\n",
    "            persistent_directory: Directory to persist the vector store\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"  \n",
    "        Initialize chromaDB client and collection\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Create persistent chromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\"Description\": \"Pdf document embedding for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector Store Initialized. Collections: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collections   : {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error Initializing Vector Store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\" \n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of langchain document\n",
    "            embeddings : Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        print(f\"Adding{len(documents)} documents to the vector store..\")\n",
    "\n",
    "        # Prepare data for chromaDB\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text =[]\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "\n",
    "\n",
    "            # Document Content\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embeddings\n",
    "\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "\n",
    "            )\n",
    "\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collections: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document to vector store {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad5cada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'../data/pdfs\\\\Dbms_Sql.pdf': 173,\n",
       "         '../data/pdfs\\\\Vinnu_ka_DAA.pdf': 20,\n",
       "         '../data/pdfs\\\\6-7Os.pdf': 3})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(doc.metadata[\"source\"] for doc in pdf_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27f9f65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 29 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape : (29, 384)\n",
      "Adding29 documents to the vector store..\n",
      "Successfully added 29 documents to vector store\n",
      "Total documents in collections: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert text to embeddings\n",
    "# split_docs = split_documents(pdf_documents)\n",
    "texts = [doc.page_content for doc in split_docs]\n",
    "\n",
    "# Generate the embeddings \n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in the vector database\n",
    "vectorstore.add_documents(split_docs,embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
